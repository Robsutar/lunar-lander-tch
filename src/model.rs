use tch::nn::{self, Adam, Linear, Module, Optimizer, OptimizerConfig, VarStore};
use tch::{Device, Kind, Tensor};

use crate::{State, StepActionEvent};

const DEVICE: Device = Device::Cpu;
type DType = f32;

/// Represents an experience tuple (洧녡洧노, 洧냢洧노, 洧녠洧노, 洧녡洧노+1) used in Deep Q-Network (DQN) algorithms.
///
/// This struct captures the agent's interaction with the environment in reinforcement learning.
/// It stores the current state (`state`), the action taken (`action`), the reward received (`reward`),
/// and the next state (`next_state`) after the action is applied. Additionally, the `done` flag indicates
/// whether the episode has terminated.
#[derive(Debug)]
pub struct Experience {
    /// The state 洧녡洧노 before the action is taken.
    pub state: State,
    /// The action 洧냢洧노 performed by the agent.
    pub action: StepActionEvent,
    /// The reward 洧녠洧노 received after taking the action.
    pub reward: DType,
    /// The resulting state 洧녡洧노+1 after the action is applied.
    pub next_state: State,
    /// A boolean flag that indicates if the episode has terminated.
    pub done: bool,
}
